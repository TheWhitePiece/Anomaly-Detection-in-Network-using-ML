{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqRPKNtSq8Ej"
      },
      "source": [
        "# data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eASIMF0PUdiO",
        "outputId": "99f4435c-c888-4800-d408-f471850004f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Monday-WorkingHours.pcap_ISCX.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a6de49ac6a3c>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Monday-WorkingHours.pcap_ISCX.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "seconds = time.time()\n",
        "%matplotlib inline\n",
        "\n",
        "number=\"0123456789\"\n",
        "\n",
        "csv_files=[\"Monday-WorkingHours.pcap_ISCX\",\n",
        "        \"Tuesday-WorkingHours.pcap_ISCX\",\n",
        "        \"Wednesday-workingHours.pcap_ISCX\",\n",
        "        \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX\",\n",
        "        \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX\",\n",
        "        \"Friday-WorkingHours-Morning.pcap_ISCX\",\n",
        "        \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX\",\n",
        "        \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX\",]\n",
        "\n",
        "main_labels=[\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Protocol\",\"Timestamp\",\"Flow Duration\",\"Total Fwd Packets\",\n",
        "   \"Total Backward Packets\",\"Total Length of Fwd Packets\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Max\",\"Fwd Packet Length Min\",\n",
        "   \"Fwd Packet Length Mean\",\"Fwd Packet Length Std\",\"Bwd Packet Length Max\",\"Bwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\n",
        "   \"Flow Bytes/s\",\"Flow Packets/s\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\n",
        "   \"Fwd IAT Min\",\"Bwd IAT Total\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\n",
        "   \"Fwd Header Length\",\"Bwd Header Length\",\"Fwd Packets/s\",\"Bwd Packets/s\",\"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Std\",\n",
        "   \"Packet Length Variance\",\"FIN Flag Count\",\"SYN Flag Count\",\"RST Flag Count\",\"PSH Flag Count\",\"ACK Flag Count\",\"URG Flag Count\",\"CWE Flag Count\",\n",
        "   \"ECE Flag Count\",\"Down/Up Ratio\",\"Average Packet Size\",\"Avg Fwd Segment Size\",\"Avg Bwd Segment Size\",\"faulty-Fwd Header Length\",\"Fwd Avg Bytes/Bulk\",\n",
        "   \"Fwd Avg Packets/Bulk\",\"Fwd Avg Bulk Rate\",\"Bwd Avg Bytes/Bulk\",\"Bwd Avg Packets/Bulk\",\"Bwd Avg Bulk Rate\",\"Subflow Fwd Packets\",\"Subflow Fwd Bytes\",\n",
        "   \"Subflow Bwd Packets\",\"Subflow Bwd Bytes\",\"Init_Win_bytes_forward\",\"Init_Win_bytes_backward\",\"act_data_pkt_fwd\",\n",
        "   \"min_seg_size_forward\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",\"Label\",\"External IP\"]\n",
        "\n",
        "main_labels2=main_labels\n",
        "main_labels=( \",\".join( i for i in main_labels ) )\n",
        "main_labels=main_labels+\"\\n\"\n",
        "flag=True\n",
        "for i in range(len(csv_files)):\n",
        "    ths = open(str(i)+\".csv\", \"w\")\n",
        "    ths.write(main_labels)\n",
        "    with open(csv_files[i]+\".csv\", \"r\") as file:\n",
        "        while True:\n",
        "            try:\n",
        "                line=file.readline()\n",
        "                if  line[0] in number:# this line eliminates the headers of CSV files and incomplete streams .\n",
        "                    if \" – \" in str(line): ##  if there is \"–\" character (\"–\", Unicode code:8211) in the flow ,  it will be chanced with \"-\" character ( Unicode code:45).\n",
        "                        line=(str(line).replace(\" – \",\" - \"))\n",
        "                    line=(str(line).replace(\"inf\",\"0\"))\n",
        "                    line=(str(line).replace(\"Infinity\",\"0\"))\n",
        "                    line=(str(line).replace(\"NaN\",\"0\"))\n",
        "\n",
        "                    ths.write(str(line))\n",
        "                else:\n",
        "                    continue\n",
        "            except:\n",
        "                break\n",
        "    ths.close()\n",
        "\n",
        "\n",
        "    df=pd.read_csv(str(i)+\".csv\",low_memory=False)\n",
        "    df=df.fillna(0)\n",
        "\n",
        "    string_features=[\"Flow Bytes/s\",\"Flow Packets/s\"]\n",
        "    for ii in string_features: #Some data in the \"Flow Bytes / s\" and \"Flow Packets / s\" columns are not numeric. Fixing this bug in this loop\n",
        "        df[ii]=df[ii].replace('Infinity', -1)\n",
        "        df[ii]=df[ii].replace('NaN', 0)\n",
        "        number_or_not=[]\n",
        "        for iii in df[ii]:\n",
        "            try:\n",
        "                k=int(float(iii))\n",
        "                number_or_not.append(int(k))\n",
        "            except:\n",
        "                number_or_not.append(iii)\n",
        "        df[ii]=number_or_not\n",
        "\n",
        "\n",
        "\n",
        "    string_features=[]\n",
        "    for j in main_labels2: # In this section, non-numeric (string and / or categorical) properties (columns) are detected.\n",
        "        if df[j].dtype==\"object\":\n",
        "            string_features.append(j)\n",
        "    try:\n",
        "        string_features.remove('Label')#The \"Label\" property was removed from the list. Because it has to remain \"categorical\" for using with different machine learning approach.\n",
        "    except:\n",
        "        print(\"error!\")\n",
        "    labelencoder_X = preprocessing.LabelEncoder()\n",
        "\n",
        "\n",
        "\n",
        "    for ii in string_features: ## In this loop, non-numeric (string and/or categorical) properties converted to numeric features.\n",
        "        try:\n",
        "            df[ii]=labelencoder_X.fit_transform(df[ii])\n",
        "        except:\n",
        "            df[ii]=df[ii].replace('Infinity', -1)\n",
        "    df=df.drop(main_labels2[61], axis=1) ## Column 61 is deleted because it is unnecessary, column 41 (\"Fwd Header Length\" feature) had be mistakenly rewritten.\n",
        "\n",
        "\n",
        "\n",
        "    ##All CSV files are merged into a single file.\n",
        "    if flag:\n",
        "        df.to_csv('all_data.csv' ,index = False)\n",
        "        flag=False\n",
        "    else:\n",
        "        df.to_csv('all_data.csv' ,index = False,header=False,mode=\"a\")\n",
        "    os.remove(str(i)+\".csv\")\n",
        "    print(\"The pre-processing phase of the \",csv_files[i],\" file is completed.\\n\")\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CJHFGiG9Lh5"
      },
      "outputs": [],
      "source": [
        "print(pd.read_csv('/content/all_data.csv').head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQTaewggq4jZ"
      },
      "source": [
        "# statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB2C9aN61a6o"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import time\n",
        "seconds = time.time()\n",
        "\n",
        "#  graph creation function\n",
        "def graph(objects,performance,x_label,y_label):\n",
        "    y_pos = np.arange(len(objects))\n",
        "    plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "    plt.yticks(y_pos, objects)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.title(y_label)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "df=pd.read_csv('all_data.csv', usecols=[\"Label\"])\n",
        "print(df.iloc[:,0].value_counts())\n",
        "a=(df.iloc[:,0].value_counts())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "key=a.keys()\n",
        "values=a.values\n",
        "small_labels=[]\n",
        "small_values=[]\n",
        "big_labels=[]\n",
        "big_values=[]\n",
        "medium_labels=[]\n",
        "medium_values=[]\n",
        "attacak=0\n",
        "benign=0\n",
        "\n",
        "\n",
        "## In this section, the attacks are grouped under 3 groups,\n",
        "## so that all values can be seen on the graph.\n",
        "for i in range(0,len(values)):\n",
        "    if values[i]>11000:\n",
        "        big_labels.append(str(key[i]))\n",
        "        big_values.append(values[i])\n",
        "    elif values[i]<600:\n",
        "        small_labels.append(str(key[i]))\n",
        "        small_values.append(values[i])\n",
        "    else:\n",
        "        medium_labels.append(str(key[i]))\n",
        "        medium_values.append(values[i])\n",
        "\n",
        "    if str(key[i])==\"BENIGN\":\n",
        "        benign+=values[i]\n",
        "    else:\n",
        "        attacak+=values[i]\n",
        "\n",
        "key =[benign,attacak]\n",
        "\n",
        "\n",
        "#functions are called to create a chartes\n",
        "labels=[\"BENIGN %\"+str(round(benign/(benign+attacak),2)*100),\n",
        "        \"ATTACK %\"+str(round(attacak/(benign+attacak),2)*100)]\n",
        "graph(big_labels,big_values,\"Numbers\",\"Attacks Labels - High-number group\")\n",
        "graph(medium_labels,medium_values,\"Numbers\",\"Attacks Labels - Medium-number group\")\n",
        "graph(small_labels,small_values,\"Numbers\",\"Attacks Labels - Small -number group\")\n",
        "graph(labels,key,\"Numbers\",\"Attack and Benign Percentage\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhmrSnmI2r21"
      },
      "source": [
        "# attack_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fmv7KPy2s7F"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "seconds = time.time()\n",
        "%matplotlib inline\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"attacks\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"Tthe folder could not be created!\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"This process may take 3 to 8 minutes, depending on the performance of your computer.\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Headers of column\n",
        "main_labels=[\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Protocol\",\"Timestamp\",\"Flow Duration\",\"Total Fwd Packets\",\n",
        "   \"Total Backward Packets\",\"Total Length of Fwd Packets\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Max\",\"Fwd Packet Length Min\",\n",
        "   \"Fwd Packet Length Mean\",\"Fwd Packet Length Std\",\"Bwd Packet Length Max\",\"Bwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\n",
        "   \"Flow Bytes/s\",\"Flow Packets/s\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\n",
        "   \"Fwd IAT Min\",\"Bwd IAT Total\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\n",
        "   \"Fwd Header Length\",\"Bwd Header Length\",\"Fwd Packets/s\",\"Bwd Packets/s\",\"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Std\",\n",
        "   \"Packet Length Variance\",\"FIN Flag Count\",\"SYN Flag Count\",\"RST Flag Count\",\"PSH Flag Count\",\"ACK Flag Count\",\"URG Flag Count\",\"CWE Flag Count\",\n",
        "   \"ECE Flag Count\",\"Down/Up Ratio\",\"Average Packet Size\",\"Avg Fwd Segment Size\",\"Avg Bwd Segment Size\",\"Fwd Avg Bytes/Bulk\",\n",
        "   \"Fwd Avg Packets/Bulk\",\"Fwd Avg Bulk Rate\",\"Bwd Avg Bytes/Bulk\",\"Bwd Avg Packets/Bulk\",\"Bwd Avg Bulk Rate\",\"Subflow Fwd Packets\",\"Subflow Fwd Bytes\",\n",
        "   \"Subflow Bwd Packets\",\"Subflow Bwd Bytes\",\"Init_Win_bytes_forward\",\"Init_Win_bytes_backward\",\"act_data_pkt_fwd\",\n",
        "   \"min_seg_size_forward\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",\"Label\",\"External IP\"]\n",
        "main_labels=( \",\".join( i for i in main_labels ) )\n",
        "\n",
        "attacks=[\"BENIGN\", \"Bot\", \"DDoS\", \"DoS GoldenEye\", \"DoS Hulk\", \"DoS Slowhttptest\", \"DoS slowloris\", \"FTP-Patator\", \"Heartbleed\", \"Infiltration\", \"PortScan\", \"SSH-Patator\", \"Web Attack – Brute Force\", \"Web Attack – Sql Injection\", \"Web Attack – XSS\"]\n",
        "folder(\"./attacks/\")\n",
        "\n",
        "benign=2359289\n",
        "\n",
        "dict_attack={\n",
        "\"Bot\":1966,\n",
        "\"DDoS\":41835,\n",
        "\"DoS GoldenEye\":10293,\n",
        "\"DoS Hulk\":231073,\n",
        "\"DoS Slowhttptest\":5499,\n",
        "\"DoS slowloris\":5796,\n",
        "\"FTP-Patator\":7938,\n",
        "\"Heartbleed\":11,\n",
        "\"Infiltration\":36,\n",
        "\"PortScan\":158930,\n",
        "\"SSH-Patator\":5897,\n",
        "\"Web Attack - Brute Force\":1507,\n",
        "\"Web Attack - XSS\":652,\n",
        "\"Web Attack - Sql Injection\":21}\n",
        "\n",
        "\n",
        "\n",
        "for i in dict_attack: # in this section, a file is opened for each attack type and is recorded at a random benign flow.\n",
        "    a,b=0,0\n",
        "    ths = open(\".\\\\attacks\\\\\"+i + \".csv\", \"w\")\n",
        "    ths.write(str(main_labels)+\"\\n\")\n",
        "    benign_num=int(benign/(dict_attack[i]*(7/3)))\n",
        "    with open(\"all_data.csv\", \"r\") as file:\n",
        "        while True:\n",
        "            try:\n",
        "                line=file.readline()\n",
        "                line=line[:-1]\n",
        "                k=line.split(\",\")\n",
        "                if k[83]==\"BENIGN\":\n",
        "                    rnd=random.randint(1,benign_num)\n",
        "                    if rnd==1:\n",
        "                            ths.write(str(line)+\"\\n\")\n",
        "                            b+=1\n",
        "                if  k[83]==i:\n",
        "                    ths.write(str(line)+\"\\n\")\n",
        "                    a+=1\n",
        "                else:\n",
        "                    continue\n",
        "            except:\n",
        "                break\n",
        "    ths.close()\n",
        "    print(i ,\"file is completed\\n attack:%d\\n benign:%d\\n\\n\\n \" %(a,b))\n",
        "\n",
        "\n",
        "##All web attack files are merged into a single file.\n",
        "webs=[\"Web Attack - Brute Force\",\"Web Attack - XSS\",\"Web Attack - Sql Injection\"]\n",
        "flag=True\n",
        "for i in webs:\n",
        "    df=pd.read_csv(\".\\\\attacks\\\\\"+str(i)+\".csv\")\n",
        "    if flag:\n",
        "        df.to_csv('.\\\\attacks\\\\Web Attack.csv' ,index = False)\n",
        "        flag=False\n",
        "    else:\n",
        "        df.to_csv('.\\\\attacks\\\\Web Attack.csv' ,index = False,header=False,mode=\"a\")\n",
        "    os.remove(\".\\\\attacks\\\\\"+str(i)+\".csv\")\n",
        "\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"operation time: = \",time.time()- seconds ,\"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5AHhcHI3dfn"
      },
      "source": [
        "\n",
        "\n",
        "# feature selection for attack files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHcgY8XF3eRB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import sklearn as sk\n",
        "import time\n",
        "seconds = time.time()\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"feaure_pics\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"The folder could not be created!\")\n",
        "\n",
        "\n",
        "# CSV files names:\n",
        "csv_files=os.listdir(\"attacks\")# It creates a list of file names in the \"attacks\" folder.\n",
        "\n",
        "# Headers of column\n",
        "main_labels=[\"Flow Duration\",\"Total Fwd Packets\",   \"Total Backward Packets\",\"Total Length of Fwd Packets\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Max\",\"Fwd Packet Length Min\",\n",
        "   \"Fwd Packet Length Mean\",\"Fwd Packet Length Std\",\"Bwd Packet Length Max\",\"Bwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\n",
        "   \"Flow Bytes/s\",\"Flow Packets/s\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\n",
        "   \"Fwd IAT Min\",\"Bwd IAT Total\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\n",
        "   \"Fwd Header Length\",\"Bwd Header Length\",\"Fwd Packets/s\",\"Bwd Packets/s\",\"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Std\",\n",
        "   \"Packet Length Variance\",\"FIN Flag Count\",\"SYN Flag Count\",\"RST Flag Count\",\"PSH Flag Count\",\"ACK Flag Count\",\"URG Flag Count\",\"CWE Flag Count\",\n",
        "   \"ECE Flag Count\",\"Down/Up Ratio\",\"Average Packet Size\",\"Avg Fwd Segment Size\",\"Avg Bwd Segment Size\",\"Fwd Avg Bytes/Bulk\",\n",
        "   \"Fwd Avg Packets/Bulk\",\"Fwd Avg Bulk Rate\",\"Bwd Avg Bytes/Bulk\",\"Bwd Avg Packets/Bulk\",\"Bwd Avg Bulk Rate\",\"Subflow Fwd Packets\",\"Subflow Fwd Bytes\",\n",
        "   \"Subflow Bwd Packets\",\"Subflow Bwd Bytes\",\"Init_Win_bytes_forward\",\"Init_Win_bytes_backward\",\"act_data_pkt_fwd\",\n",
        "   \"min_seg_size_forward\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\n",
        "    \"Idle Mean\",\"Idle Std\",\"Idle Max\", \"Idle Min\",\"Label\"]\n",
        "\n",
        "ths = open(\"importance_list_for_attack_files.csv\", \"w\")\n",
        "folder(\"./feaure_pics/\")\n",
        "for j in csv_files:\n",
        "    df=pd.read_csv(\".\\\\attacks\\\\\"+j,usecols=main_labels)\n",
        "    df=df.fillna(0)\n",
        "    attack_or_not=[]\n",
        "    for i in df[\"Label\"]:#it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "        if i ==\"BENIGN\":\n",
        "            attack_or_not.append(1)\n",
        "        else:\n",
        "            attack_or_not.append(0)\n",
        "    df[\"Label\"]=attack_or_not\n",
        "\n",
        "    y = df[\"Label\"].values\n",
        "    del df[\"Label\"]\n",
        "    X = df.values\n",
        "\n",
        "    X = np.float32(X)\n",
        "    X[np.isnan(X)] = 0\n",
        "    X[np.isinf(X)] = 0\n",
        "\n",
        "\n",
        "    #computing the feature importances\n",
        "    forest = sk.ensemble.RandomForestRegressor(n_estimators=250,random_state=0)\n",
        "    forest.fit(X, y)\n",
        "    importances = forest.feature_importances_\n",
        "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "                 axis=0)\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    refclasscol=list(df.columns.values)\n",
        "    impor_bars = pd.DataFrame({'Features':refclasscol[0:20],'importance':importances[0:20]})\n",
        "    impor_bars = impor_bars.sort_values('importance',ascending=False).set_index('Features')\n",
        "    plt.rcParams['figure.figsize'] = (10, 5)\n",
        "    impor_bars.plot.bar();\n",
        "    #printing the feature importances\n",
        "    count=0\n",
        "    fea_ture=j[0:-4]+\"=[\"\n",
        "    for i in impor_bars.index:\n",
        "        fea_ture=fea_ture+\"\\\"\"+str(i)+\"\\\",\"\n",
        "        count+=1\n",
        "        if count==5:\n",
        "            fea_ture=fea_ture[0:-1]+\"]\"\n",
        "            break\n",
        "    print(j[0:-4],\"importance list:\")\n",
        "    print(j[0:-4],\"\\n\",impor_bars.head(20),\"\\n\\n\\n\")\n",
        "    print(fea_ture)\n",
        "    plt.title(j[0:-4]+\" Attack - Feature Importance\")\n",
        "    plt.ylabel('Importance')\n",
        "    plt.savefig(\"./feaure_pics/\"+j[0:-4]+\".pdf\",bbox_inches='tight', papertype = 'a4', orientation = 'portrait', format = 'pdf')\n",
        "    ths.write((  fea_ture ) )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\\n\\n\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")\n",
        "ths.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA7bb8Dt3h7F"
      },
      "source": [
        "# feature selection for all data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFRwMFHT3iuO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import sklearn as sk\n",
        "import time\n",
        "seconds = time.time()\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"feaure_pics\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"The folder could not be created!\")\n",
        "\n",
        "\n",
        "# CSV files names:\n",
        "csv_files=[\"all_data.csv\"]# It creates a list of file names in the \"attacks\" folder.\n",
        "\n",
        "# Headers of column\n",
        "main_labels=[\"Flow Duration\",\"Total Fwd Packets\",   \"Total Backward Packets\",\"Total Length of Fwd Packets\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Max\",\"Fwd Packet Length Min\",\n",
        "   \"Fwd Packet Length Mean\",\"Fwd Packet Length Std\",\"Bwd Packet Length Max\",\"Bwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\n",
        "   \"Flow Bytes/s\",\"Flow Packets/s\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\n",
        "   \"Fwd IAT Min\",\"Bwd IAT Total\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\n",
        "   \"Fwd Header Length\",\"Bwd Header Length\",\"Fwd Packets/s\",\"Bwd Packets/s\",\"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Std\",\n",
        "   \"Packet Length Variance\",\"FIN Flag Count\",\"SYN Flag Count\",\"RST Flag Count\",\"PSH Flag Count\",\"ACK Flag Count\",\"URG Flag Count\",\"CWE Flag Count\",\n",
        "   \"ECE Flag Count\",\"Down/Up Ratio\",\"Average Packet Size\",\"Avg Fwd Segment Size\",\"Avg Bwd Segment Size\",\"Fwd Avg Bytes/Bulk\",\n",
        "   \"Fwd Avg Packets/Bulk\",\"Fwd Avg Bulk Rate\",\"Bwd Avg Bytes/Bulk\",\"Bwd Avg Packets/Bulk\",\"Bwd Avg Bulk Rate\",\"Subflow Fwd Packets\",\"Subflow Fwd Bytes\",\n",
        "   \"Subflow Bwd Packets\",\"Subflow Bwd Bytes\",\"Init_Win_bytes_forward\",\"Init_Win_bytes_backward\",\"act_data_pkt_fwd\",\n",
        "   \"min_seg_size_forward\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\n",
        "    \"Idle Mean\",\"Idle Std\",\"Idle Max\", \"Idle Min\",\"Label\"]\n",
        "\n",
        "ths = open(\"importance_list_all_data.csv\", \"w\")\n",
        "folder(\"./feaure_pics/\")\n",
        "for j in csv_files:\n",
        "    df=pd.read_csv(j,usecols=main_labels)\n",
        "    df=df.fillna(0)\n",
        "    attack_or_not=[]\n",
        "    for i in df[\"Label\"]:#it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "        if i ==\"BENIGN\":\n",
        "            attack_or_not.append(1)\n",
        "        else:\n",
        "            attack_or_not.append(0)\n",
        "    df[\"Label\"]=attack_or_not\n",
        "\n",
        "    y = df[\"Label\"].values\n",
        "    del df[\"Label\"]\n",
        "    X = df.values\n",
        "\n",
        "\n",
        "    X = np.float32(X)\n",
        "    X[np.isnan(X)] = 0\n",
        "    X[np.isinf(X)] = 0\n",
        "\n",
        "\n",
        "    #computing the feature importances\n",
        "    forest = sk.ensemble.RandomForestRegressor(n_estimators=250,random_state=0)\n",
        "    forest.fit(X, y)\n",
        "    importances = forest.feature_importances_\n",
        "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "                 axis=0)\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    refclasscol=list(df.columns.values)\n",
        "    impor_bars = pd.DataFrame({'Features':refclasscol[0:20],'importance':importances[0:20]})\n",
        "    impor_bars = impor_bars.sort_values('importance',ascending=False).set_index('Features')\n",
        "    plt.rcParams['figure.figsize'] = (10, 5)\n",
        "    impor_bars.plot.bar();\n",
        "    #printing the feature importances\n",
        "    count=0\n",
        "    fea_ture=j[0:-4]+\"=[\"\n",
        "    for i in impor_bars.index:\n",
        "        fea_ture=fea_ture+\"\\\"\"+str(i)+\"\\\",\"\n",
        "        count+=1\n",
        "        if count==5:\n",
        "            fea_ture=fea_ture[0:-1]+\"]\"\n",
        "            break\n",
        "    print(j[0:-4],\"importance list:\")\n",
        "    print(j[0:-4],\"\\n\",impor_bars.head(20),\"\\n\\n\\n\")\n",
        "    print(fea_ture)\n",
        "    plt.title(j[0:-4]+\" Attack - Feature Importance\")\n",
        "    plt.ylabel('Importance')\n",
        "    plt.savefig(\"./feaure_pics/\"+j[0:-4]+\".pdf\",bbox_inches='tight', papertype = 'a4', orientation = 'portrait', format = 'pdf')\n",
        "    ths.write((  fea_ture ) )\n",
        "    plt.tight_layout()\n",
        "    #plt.show()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\\n\\n\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"secomds\")\n",
        "ths.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW2179Pk3sCt"
      },
      "source": [
        "# machine learning implementation for attack files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol1BOnNF4Br1"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "result=\"./results/results_1.csv\" #a CSV file is named in which the results are saved.\n",
        "csv_files=os.listdir(\"attacks\")# CSV files names: #The names of the files in the attacks folder are taken and assigned to a list (csv_files).\n",
        "path=\".\\\\attacks\\\\\"\n",
        "repetition=10\n",
        "\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"results\" and \"result_graph_1\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"The folder could not be created!\")\n",
        "\n",
        "folder_name=\"./results/\"\n",
        "folder(folder_name)\n",
        "folder_name=\"./results/result_graph_1/\"\n",
        "folder(folder_name)\n",
        "\n",
        "\n",
        "#The machine learning algorithms to be used are defined in a dictionary (ml_list).\n",
        "ml_list={\n",
        "\"Naive Bayes\":GaussianNB(),\n",
        "\"QDA\":QDA(),\n",
        "\"Random Forest\":RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "\"ID3\" :DecisionTreeClassifier(max_depth=5,criterion=\"entropy\"),\n",
        "\"AdaBoost\":AdaBoostClassifier(),\n",
        "\"MLP\":MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500),\n",
        "\"Nearest Neighbors\":KNeighborsClassifier(3)}\n",
        "\n",
        "\n",
        "\n",
        "# the features to be used for each attack type is defined in a dictionary(features).\n",
        "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
        "features={\"Bot\":[\"Bwd Packet Length Mean\",\"Flow IAT Max\",\"Flow Duration\",\"Flow IAT Min\",\"Label\"],\n",
        "\"DDoS\":[\"Bwd Packet Length Std\",\"Total Backward Packets\",\"Fwd IAT Total\",\"Flow Duration\",\"Label\"],\n",
        "\"DoS GoldenEye\":[\"Flow IAT Max\",\"Bwd Packet Length Std\",\"Flow IAT Min\",\"Total Backward Packets\",\"Label\"],\n",
        "\"DoS Hulk\":[\"Bwd Packet Length Std\",\"Fwd Packet Length Std\",\"Fwd Packet Length Max\",\"Flow IAT Min\",\"Label\"],\n",
        "\"DoS Slowhttptest\":[\"Flow IAT Mean\",\"Fwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Total Length of Bwd Packets\",\"Label\"],\n",
        "\"DoS slowloris\":[\"Flow IAT Mean\",\"Total Length of Bwd Packets\",\"Bwd Packet Length Mean\",\"Total Fwd Packets\",\"Label\"],\n",
        "\"FTP-Patator\":[\"Fwd Packet Length Max\",\"Fwd Packet Length Std\",\"Fwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Label\"],\n",
        "\"Heartbleed\":[\"Total Backward Packets\",\"Fwd Packet Length Max\",\"Flow IAT Min\",\"Bwd Packet Length Max\",\"Label\"],\n",
        "\"Infiltration\":[\"Fwd Packet Length Max\",\"Fwd Packet Length Mean\",\"Flow Duration\",\"Total Length of Fwd Packets\",\"Label\"],\n",
        "\"PortScan\":[\"Flow Bytes/s\",\"Total Length of Fwd Packets\",\"Fwd IAT Total\",\"Flow Duration\",\"Label\"],\n",
        "\"SSH-Patator\":[\"Fwd Packet Length Max\",\"Flow Duration\",\"Flow IAT Max\",\"Total Length of Fwd Packets\",\"Label\"],\n",
        "\"Web Attack\":[\"Bwd Packet Length Std\",\"Total Length of Fwd Packets\",\"Flow Bytes/s\",\"Flow IAT Max\",\"Label\"]}\n",
        "\n",
        "seconds=time.time()#time stamp for all processing time\n",
        "\n",
        "\n",
        "\n",
        "with open(result, \"w\", newline=\"\",encoding=\"utf-8\") as f:#a CSV file is created to save the results obtained.\n",
        "    wrt = csv.writer(f)\n",
        "    wrt.writerow([\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
        "    print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
        "    a=[]\n",
        "\n",
        "    feature_list=list(features[j[0:-4]])\n",
        "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
        "    df=df.fillna(0)\n",
        "    attack_or_not=[]\n",
        "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "\n",
        "        if i ==\"BENIGN\":\n",
        "            attack_or_not.append(1)\n",
        "        else:\n",
        "            attack_or_not.append(0)\n",
        "    df[\"Label\"]=attack_or_not\n",
        "\n",
        "\n",
        "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X\n",
        "    del df[\"Label\"]\n",
        "    feature_list.remove('Label')\n",
        "    X = df[feature_list]\n",
        "\n",
        "\n",
        "    for ii in ml_list: #this loop runs on the list containing the machine learning algorithm names. Operations are repeated for all the 7 algorithm\n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        accuracy=[]\n",
        "        t_time=[]\n",
        "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
        "            second=time.time()#time stamp for processing time\n",
        "\n",
        "            # cross-validation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test).\n",
        "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
        "\n",
        "\n",
        "            #machine learning algorithm is applied in this section\n",
        "            clf = ml_list[ii]#choose algorithm from ml_list dictionary\n",
        "            clf.fit(X_train, y_train)\n",
        "            predict =clf.predict(X_test)\n",
        "\n",
        "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.\n",
        "\n",
        "            f_1=f1_score(y_test, predict, average='macro')\n",
        "            pr=precision_score(y_test, predict, average='macro')\n",
        "            rc=recall_score(y_test, predict, average='macro')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            precision.append(float(pr))\n",
        "            recall.append(float(rc))\n",
        "            f1.append(float(f_1))\n",
        "            accuracy.append(clf.score(X_test, y_test))\n",
        "            t_time.append(float((time.time()-second)) )\n",
        "\n",
        "\n",
        "\n",
        "        print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)),\n",
        "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
        "\n",
        "        with open(result, \"a\", newline=\"\",encoding=\"utf-8\") as f: # all the values found are saved in the opened file.\n",
        "            wrt = csv.writer(f)\n",
        "            for i in range(0,len(t_time)):\n",
        "                wrt.writerow([j[0:-4],ii,accuracy[i],precision[i],recall[i],f1[i],t_time[i]])#file name, algorithm name, precision, recall and f-measure are writed in CSV file\n",
        "        a.append(f1)\n",
        "\n",
        "\n",
        "     # In this section, Box graphics are created for the results of machine learning algorithms and saved in the feaure_graph folder.\n",
        "\n",
        "\n",
        "    ml=[\"Naive Bayes\",\"QDA\",\"Random Forest\",\"ID3\",\"AdaBoost\",\"MLP\",\"Nearest Neighbors\"]\n",
        "    temp=0\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(12, 6), sharey=True)\n",
        "    for c in range(2):\n",
        "        for b in range(4):\n",
        "            axes[c, b].boxplot(a[temp] )\n",
        "            axes[c, b].set_title(str(j[0:-4])+\" - \"+str(ml[temp]),fontsize=7)\n",
        "            axes[c, b].set_ylabel((\"F measure\"))\n",
        "            temp+=1\n",
        "            if temp==7:\n",
        "                break\n",
        "        if temp==7:\n",
        "            break\n",
        "    plt.savefig(folder_name+j[0:-4]+\".pdf\",bbox_inches='tight', papertype = 'a4', orientation = 'portrait', format = 'pdf')\n",
        "    plt.show()\n",
        "    print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYMWn8eh4C7k"
      },
      "source": [
        "# machine learning implementation with 18 feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBwdUxkP4Di8"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "result=\"./results/results_1.csv\" #a CSV file is named in which the results are saved.\n",
        "csv_files=os.listdir(\"attacks\")# CSV files names: #The names of the files in the attacks folder are taken and assigned to a list (csv_files).\n",
        "path=\".\\\\attacks\\\\\"\n",
        "repetition=10\n",
        "\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"results\" and \"result_graph_1\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"The folder could not be created!\")\n",
        "\n",
        "folder_name=\"./results/\"\n",
        "folder(folder_name)\n",
        "folder_name=\"./results/result_graph_1/\"\n",
        "folder(folder_name)\n",
        "\n",
        "\n",
        "#The machine learning algorithms to be used are defined in a dictionary (ml_list).\n",
        "ml_list={\n",
        "\"Naive Bayes\":GaussianNB(),\n",
        "\"QDA\":QDA(),\n",
        "\"Random Forest\":RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "\"ID3\" :DecisionTreeClassifier(max_depth=5,criterion=\"entropy\"),\n",
        "\"AdaBoost\":AdaBoostClassifier(),\n",
        "\"MLP\":MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500),\n",
        "\"Nearest Neighbors\":KNeighborsClassifier(3)}\n",
        "\n",
        "\n",
        "\n",
        "# the features to be used for each attack type is defined in a dictionary(features).\n",
        "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
        "features={\"Bot\":[\"Bwd Packet Length Mean\",\"Flow IAT Max\",\"Flow Duration\",\"Flow IAT Min\",\"Label\"],\n",
        "\"DDoS\":[\"Bwd Packet Length Std\",\"Total Backward Packets\",\"Fwd IAT Total\",\"Flow Duration\",\"Label\"],\n",
        "\"DoS GoldenEye\":[\"Flow IAT Max\",\"Bwd Packet Length Std\",\"Flow IAT Min\",\"Total Backward Packets\",\"Label\"],\n",
        "\"DoS Hulk\":[\"Bwd Packet Length Std\",\"Fwd Packet Length Std\",\"Fwd Packet Length Max\",\"Flow IAT Min\",\"Label\"],\n",
        "\"DoS Slowhttptest\":[\"Flow IAT Mean\",\"Fwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Total Length of Bwd Packets\",\"Label\"],\n",
        "\"DoS slowloris\":[\"Flow IAT Mean\",\"Total Length of Bwd Packets\",\"Bwd Packet Length Mean\",\"Total Fwd Packets\",\"Label\"],\n",
        "\"FTP-Patator\":[\"Fwd Packet Length Max\",\"Fwd Packet Length Std\",\"Fwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Label\"],\n",
        "\"Heartbleed\":[\"Total Backward Packets\",\"Fwd Packet Length Max\",\"Flow IAT Min\",\"Bwd Packet Length Max\",\"Label\"],\n",
        "\"Infiltration\":[\"Fwd Packet Length Max\",\"Fwd Packet Length Mean\",\"Flow Duration\",\"Total Length of Fwd Packets\",\"Label\"],\n",
        "\"PortScan\":[\"Flow Bytes/s\",\"Total Length of Fwd Packets\",\"Fwd IAT Total\",\"Flow Duration\",\"Label\"],\n",
        "\"SSH-Patator\":[\"Fwd Packet Length Max\",\"Flow Duration\",\"Flow IAT Max\",\"Total Length of Fwd Packets\",\"Label\"],\n",
        "\"Web Attack\":[\"Bwd Packet Length Std\",\"Total Length of Fwd Packets\",\"Flow Bytes/s\",\"Flow IAT Max\",\"Label\"]}\n",
        "\n",
        "seconds=time.time()#time stamp for all processing time\n",
        "\n",
        "\n",
        "\n",
        "with open(result, \"w\", newline=\"\",encoding=\"utf-8\") as f:#a CSV file is created to save the results obtained.\n",
        "    wrt = csv.writer(f)\n",
        "    wrt.writerow([\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
        "    print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
        "    a=[]\n",
        "\n",
        "    feature_list=list(features[j[0:-4]])\n",
        "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
        "    df=df.fillna(0)\n",
        "    attack_or_not=[]\n",
        "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "\n",
        "        if i ==\"BENIGN\":\n",
        "            attack_or_not.append(1)\n",
        "        else:\n",
        "            attack_or_not.append(0)\n",
        "    df[\"Label\"]=attack_or_not\n",
        "\n",
        "\n",
        "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X\n",
        "    del df[\"Label\"]\n",
        "    feature_list.remove('Label')\n",
        "    X = df[feature_list]\n",
        "\n",
        "\n",
        "    for ii in ml_list: #this loop runs on the list containing the machine learning algorithm names. Operations are repeated for all the 7 algorithm\n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        accuracy=[]\n",
        "        t_time=[]\n",
        "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
        "            second=time.time()#time stamp for processing time\n",
        "\n",
        "            # cross-validation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test).\n",
        "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
        "\n",
        "\n",
        "            #machine learning algorithm is applied in this section\n",
        "            clf = ml_list[ii]#choose algorithm from ml_list dictionary\n",
        "            clf.fit(X_train, y_train)\n",
        "            predict =clf.predict(X_test)\n",
        "\n",
        "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.\n",
        "\n",
        "            f_1=f1_score(y_test, predict, average='macro')\n",
        "            pr=precision_score(y_test, predict, average='macro')\n",
        "            rc=recall_score(y_test, predict, average='macro')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            precision.append(float(pr))\n",
        "            recall.append(float(rc))\n",
        "            f1.append(float(f_1))\n",
        "            accuracy.append(clf.score(X_test, y_test))\n",
        "            t_time.append(float((time.time()-second)) )\n",
        "\n",
        "\n",
        "\n",
        "        print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)),\n",
        "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
        "\n",
        "        with open(result, \"a\", newline=\"\",encoding=\"utf-8\") as f: # all the values found are saved in the opened file.\n",
        "            wrt = csv.writer(f)\n",
        "            for i in range(0,len(t_time)):\n",
        "                wrt.writerow([j[0:-4],ii,accuracy[i],precision[i],recall[i],f1[i],t_time[i]])#file name, algorithm name, precision, recall and f-measure are writed in CSV file\n",
        "        a.append(f1)\n",
        "\n",
        "\n",
        "     # In this section, Box graphics are created for the results of machine learning algorithms and saved in the feaure_graph folder.\n",
        "\n",
        "\n",
        "    ml=[\"Naive Bayes\",\"QDA\",\"Random Forest\",\"ID3\",\"AdaBoost\",\"MLP\",\"Nearest Neighbors\"]\n",
        "    temp=0\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(12, 6), sharey=True)\n",
        "    for c in range(2):\n",
        "        for b in range(4):\n",
        "            axes[c, b].boxplot(a[temp] )\n",
        "            axes[c, b].set_title(str(j[0:-4])+\" - \"+str(ml[temp]),fontsize=7)\n",
        "            axes[c, b].set_ylabel((\"F measure\"))\n",
        "            temp+=1\n",
        "            if temp==7:\n",
        "                break\n",
        "        if temp==7:\n",
        "            break\n",
        "    plt.savefig(folder_name+j[0:-4]+\".pdf\",bbox_inches='tight', papertype = 'a4', orientation = 'portrait', format = 'pdf')\n",
        "    plt.show()\n",
        "    print(\"\\n------------------------------------------------------------------------------------------------------\\n\\n\")\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF1wmABt4fpS"
      },
      "source": [
        "# machine learning implementation with 7 feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BSJYHu44grS"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "result=\"./results/results_3.csv\" #a CSV file is named in which the results are saved.\n",
        "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
        "path=\"\"\n",
        "repetition=10\n",
        "\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"results\" and \"result_graph_1\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"The folder could not be created!\")\n",
        "\n",
        "folder_name=\"./results/\"\n",
        "folder(folder_name)\n",
        "folder_name=\"./results/result_graph_3/\"\n",
        "folder(folder_name)\n",
        "\n",
        "\n",
        "#The machine learning algorithms to be used are defined in a dictionary (ml_list).\n",
        "ml_list={\n",
        "\"Naive Bayes\":GaussianNB(),\n",
        "\"QDA\":QDA(),\n",
        "\"Random Forest\":RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "\"ID3\" :DecisionTreeClassifier(max_depth=5,criterion=\"entropy\"),\n",
        "\"AdaBoost\":AdaBoostClassifier(),\n",
        "\"MLP\":MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500),\n",
        "\"Nearest Neighbors\":KNeighborsClassifier(3)}\n",
        "\n",
        "\n",
        "\n",
        "#list of all columns to be imported\n",
        "# the 7 features with the highest importance weight selected by the file \"04_2_feature_selection_for_attack_files.py\" are used here. (+ Label Feature)\n",
        "\n",
        "features={\"all_data\":[\"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Total Length of Fwd Packets\", \"Fwd Packet Length Std\",\n",
        "     \"Flow IAT Std\", \"Flow IAT Min\", \"Fwd IAT Total\",\"Label\"]}\n",
        "\n",
        "seconds=time.time()#time stamp for all processing time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(result, \"w\", newline=\"\",encoding=\"utf-8\") as f:#a CSV file is created to save the results obtained.\n",
        "    wrt = csv.writer(f)\n",
        "    wrt.writerow([\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
        "    print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
        "    feature_list=list(features[j[0:-4]])\n",
        "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
        "    df=df.fillna(0)\n",
        "    attack_or_not=[]\n",
        "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "        if i ==\"BENIGN\":\n",
        "            attack_or_not.append(1)\n",
        "        else:\n",
        "            attack_or_not.append(0)\n",
        "    df[\"Label\"]=attack_or_not\n",
        "\n",
        "\n",
        "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X\n",
        "    del df[\"Label\"]\n",
        "    feature_list.remove('Label')\n",
        "    X = df[feature_list]\n",
        "\n",
        "\n",
        "    for ii in ml_list: #this loop runs on the list containing the machine learning algorithm names. Operations are repeated for all the 7 algorithm\n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        accuracy=[]\n",
        "        t_time=[]\n",
        "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
        "            second=time.time()#time stamp for processing time\n",
        "\n",
        "            # cross-validation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test).\n",
        "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
        "\n",
        "            #machine learning algorithm is applied in this section\n",
        "            clf = ml_list[ii]#choose algorithm from ml_list dictionary\n",
        "            clf.fit(X_train, y_train)\n",
        "            predict =clf.predict(X_test)\n",
        "\n",
        "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.\n",
        "\n",
        "            f_1=f1_score(y_test, predict, average='macro')\n",
        "            pr=precision_score(y_test, predict, average='macro')\n",
        "            rc=recall_score(y_test, predict, average='macro')\n",
        "\n",
        "            precision.append(float(pr))\n",
        "            recall.append(float(rc))\n",
        "            f1.append(float(f_1))\n",
        "            accuracy.append(clf.score(X_test, y_test))\n",
        "            t_time.append(float((time.time()-second)) )\n",
        "\n",
        "\n",
        "\n",
        "        print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)),\n",
        "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the result of the ten repetitions is printed on the screen.\n",
        "\n",
        "        with open(result, \"a\", newline=\"\",encoding=\"utf-8\") as f: # all the values found are saved in the opened file.\n",
        "            wrt = csv.writer(f)\n",
        "            for i in range(0,len(t_time)):\n",
        "                wrt.writerow([j[0:-4],ii,accuracy[i],precision[i],recall[i],f1[i],t_time[i]])#file name, algorithm name, precision, recall and f-measure are writed in CSV file\n",
        "\n",
        "     # In this section, Box graphics are created for the results of machine learning algorithms and saved in the feaure_graph folder.\n",
        "        plt.boxplot(f1)\n",
        "        plt.title(\"All Dataset - \" +str(ii))\n",
        "        plt.ylabel('F-measure')\n",
        "        plt.savefig(folder_name+j[0:-4]+str(ii)+\".pdf\",bbox_inches='tight', papertype = 'a4', orientation = 'portrait', format = 'pdf')\n",
        "        plt.show()# you can remove the # sign if you want to see the graphics simultaneously\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfBHfA3a47TT"
      },
      "source": [
        "# machine learning implementation final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05mLd0oz48Sc"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"feaure_graph\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"The folder could not be created!\")\n",
        "\n",
        "\n",
        "result=\"./results/results_Final.csv\" #a CSV file is named in which the results are saved.\n",
        "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
        "path=\"\"\n",
        "repetition=10\n",
        "\n",
        "\n",
        "def folder(f_name): #this function creates a folder named \"results\" and \"result_graph_1\" in the program directory.\n",
        "    try:\n",
        "        if not os.path.exists(f_name):\n",
        "            os.makedirs(f_name)\n",
        "    except OSError:\n",
        "        print (\"The folder could not be created!\")\n",
        "\n",
        "folder_name=\"./results/\"\n",
        "folder(folder_name)\n",
        "folder_name=\"./results/result_graph_Final/\"\n",
        "folder(folder_name)\n",
        "\n",
        "\n",
        "\n",
        "# the 20 features selected by the file \"04_2_feature_selection_for_attack_files.py\" are used here. (+ Label Feature)\n",
        "usecols=[\"Bwd Packet Length Std\",\"Flow Bytes/s\",\"Total Length of Fwd Packets\",\"Fwd Packet Length Std\",\"Flow IAT Std\",\n",
        "\"Flow IAT Min\",\"Fwd IAT Total\",\"Flow Duration\",\"Bwd Packet Length Max\",\"Flow IAT Max\",\"Flow IAT Mean\",\"Total Length of Bwd Packets\",\n",
        "\"Fwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Flow Packets/s\",\"Fwd Packet Length Mean\",\"Total Backward Packets\",\"Total Fwd Packets\",\n",
        "\"Fwd Packet Length Max\",\"Bwd Packet Length Min\",'Label']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#The machine learning algorithms to be used are defined in a dictionary (ml_list).\n",
        "ml_list={\n",
        "\"Naive Bayes\":GaussianNB(),\n",
        "\"QDA\":QDA(),\n",
        "\"MLP\":MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500),\n",
        "\"Random Forest\":RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "\"ID3\" :DecisionTreeClassifier(max_depth=5,criterion=\"entropy\"),\n",
        "\"AdaBoost\":AdaBoostClassifier(),\n",
        "\"Nearest Neighbors\":KNeighborsClassifier(3)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# the features to be used for Random Forest,ID3,AdaBoost,Nearest Neighbors is defined in a list(others).\n",
        "# the first 7 of the features created by the file \"04_2_feature_selection_for_attack_files.py\" are used here.\n",
        "others=[\"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Total Length of Fwd Packets\", \"Fwd Packet Length Std\",\n",
        "     \"Flow IAT Std\", \"Flow IAT Min\", \"Fwd IAT Total\"]\n",
        "\n",
        "#In this part different sets of properties for machine learning methods are defined as follows:\n",
        "#For \"Naive Bayes\", \"QDA\", and \"MLP\", each method has a different feature list.\n",
        "#other algorithms (Random Forest, ID3, AdaBoost, and Nearest Neighbors) use\n",
        "#the first 7 of the features created by the file \"04_2_feature_selection_for_attack_files.py\" are used here.\n",
        "algorithms_features={\"Naive Bayes\":['Bwd Packet Length Std', 'Total Length of Fwd Packets', 'Flow IAT Min', 'Fwd Packet Length Min', 'Flow Packets/s', 'Fwd Packet Length Mean'] ,\n",
        "\"QDA\":['Bwd Packet Length Std', 'Flow Bytes/s', 'Total Length of Fwd Packets', 'Flow IAT Min'],\n",
        "\"MLP\":['Bwd Packet Length Std', 'Flow Bytes/s', 'Total Length of Fwd Packets', 'Fwd Packet Length Std',\n",
        "'Flow IAT Min', 'Bwd Packet Length Max','Fwd Packet Length Min', 'Bwd Packet Length Mean',\n",
        "'Total Backward Packets', 'Total Fwd Packets', 'Fwd Packet Length Max', 'Bwd Packet Length Min'],\n",
        "\n",
        "\n",
        "#these algorithms use the features in the \"others\" list\n",
        "\"Random Forest\":others,\n",
        "\"ID3\" :others,\n",
        "\"AdaBoost\":others,\n",
        "\"Nearest Neighbors\":others}\n",
        "\n",
        "seconds=time.time()#time stamp for all processing time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(result, \"w\", newline=\"\",encoding=\"utf-8\") as f:#a CSV file is created to save the results obtained.\n",
        "    wrt = csv.writer(f)\n",
        "    wrt.writerow([\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"])\n",
        "\n",
        "for j in csv_files: #this loop runs on the list containing the filenames.Operations are repeated for all attack files\n",
        "    print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
        "    feature_list=usecols\n",
        "    df=pd.read_csv(path+j,usecols=feature_list)#read an attack file.\n",
        "    df=df.fillna(0)\n",
        "    attack_or_not=[]\n",
        "    for i in df[\"Label\"]: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "        if i ==\"BENIGN\":\n",
        "            attack_or_not.append(1)\n",
        "        else:\n",
        "            attack_or_not.append(0)\n",
        "    df[\"Label\"]=attack_or_not\n",
        "\n",
        "\n",
        "    y = df[\"Label\"] #this section separates the label and the data into two separate pieces, as Label=y Data=X\n",
        "    del df[\"Label\"]\n",
        "    feature_list.remove('Label')\n",
        "    for ii in ml_list: #this loop runs on the list containing the machine learning algorithm names. Operations are repeated for all the 7 algorithm\n",
        "        X = df[algorithms_features[ii]]\n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        accuracy=[]\n",
        "        t_time=[]\n",
        "        for i in range(repetition): # This loop allows cross-validation and machine learning algorithm to be repeated 10 times\n",
        "            second=time.time()#time stamp for processing time\n",
        "\n",
        "            # cross-validation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y,#  data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm.\n",
        "                test_size = 0.20, random_state = repetition)#  So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
        "\n",
        "\n",
        "            #machine learning algorithm is applied in this section\n",
        "            clf = ml_list[ii]#choose algorithm from ml_list dictionary\n",
        "            clf.fit(X_train, y_train)\n",
        "            predict =clf.predict(X_test)\n",
        "\n",
        "            #makes \"classification report\" and assigns the precision, f-measure, and recall values.s.\n",
        "\n",
        "\n",
        "            f_1=f1_score(y_test, predict, average='macro')\n",
        "            pr=precision_score(y_test, predict, average='macro')\n",
        "            rc=recall_score(y_test, predict, average='macro')\n",
        "\n",
        "\n",
        "\n",
        "            precision.append(float(pr))\n",
        "            recall.append(float(rc))\n",
        "            f1.append(float(f_1))\n",
        "            accuracy.append(clf.score(X_test, y_test))\n",
        "            t_time.append(float((time.time()-second)) )\n",
        "\n",
        "\n",
        "\n",
        "        print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)),\n",
        "            str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))#the avarage result of the ten repetitions is printed on the screen.\n",
        "\n",
        "        with open(result, \"a\", newline=\"\",encoding=\"utf-8\") as f: # all the values found are saved in the opened file.\n",
        "            wrt = csv.writer(f)\n",
        "            for i in range(0,len(t_time)):\n",
        "                wrt.writerow([j[0:-4],ii,accuracy[i],precision[i],recall[i],f1[i],t_time[i]])#file name, algorithm name, precision, recall and f-measure are writed in CSV file\n",
        "\n",
        "\n",
        "\n",
        "        # In this section, Box graphics are created for the results of machine learning algorithms and saved in the feaure_graph folder.\n",
        "        plt.boxplot(f1)\n",
        "        plt.title(\"All Dataset - \" +str(ii))\n",
        "        plt.ylabel('F-measure')\n",
        "        plt.savefig(folder_name+j[0:-4]+str(ii)+\".pdf\",bbox_inches='tight', papertype = 'a4', orientation = 'portrait', format = 'pdf')\n",
        "        plt.show()# you can remove the # sign if you want to see the graphics simultaneously\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E74vN_a49bR"
      },
      "source": [
        "# ml f-measure comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MQg1tN85Lmf"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import time\n",
        "seconds = time.time()\n",
        "\n",
        "\n",
        "#list of all columns to be imported\n",
        "# the 20 features selected by the file \"04_2_feature_selection_for_attack_files.py\" are used here. (+ Label Feature)\n",
        "features=[\"Bwd Packet Length Std\",\"Flow Bytes/s\",\"Total Length of Fwd Packets\",\"Fwd Packet Length Std\",\n",
        "\"Flow IAT Std\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Flow Duration\",\"Bwd Packet Length Max\",\"Flow IAT Max\",\n",
        "\"Flow IAT Mean\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Min\",\"Bwd Packet Length Mean\",\n",
        "\"Flow Packets/s\",\"Fwd Packet Length Mean\",\"Total Backward Packets\",\"Total Fwd Packets\",\"Fwd Packet Length Max\",\n",
        "\"Bwd Packet Length Min\",'Label']\n",
        "\n",
        "df=pd.read_csv('all_data.csv',usecols=features)#CSV rading\n",
        "\n",
        "\n",
        "\n",
        "print ('%-17s %-17s ' % (\"Feature Number\",\"Feature\"))# print output header\n",
        "for i in range(len(features)-1):\n",
        "    print ('%-17s %-17s' % (i+1,features[i]))# print features  and feature numbers\n",
        "\n",
        "\n",
        "print ('\\n\\n\\n')\n",
        "\n",
        "attack_or_not=[]\n",
        "for i in df.iloc[:,-1]:\n",
        "    if i ==\"BENIGN\":#it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "        attack_or_not.append(1)\n",
        "    else:\n",
        "        attack_or_not.append(0)\n",
        "df.iloc[:,-1]=attack_or_not\n",
        "y = df.iloc[:, -1].values #labes-y\n",
        "my_list=[]\n",
        "\n",
        "\n",
        "least=0\n",
        "\n",
        "\n",
        "\n",
        "ml_list={#The machine learning algorithms to be used are defined in a dictionary (ml_list).\n",
        "\"Naive Bayes\":GaussianNB(),\n",
        "\"QDA\":QDA(),\n",
        "##\"Random Forest\":RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "##\"ID3\" :DecisionTreeClassifier(max_depth=5,criterion=\"entropy\"),\n",
        "##\"AdaBoost\":AdaBoostClassifier(),\n",
        "##\"Nearest Neighbors\":KNeighborsClassifier(3),\n",
        "\"MLP\":MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)}\n",
        "\n",
        "\n",
        "features.pop()#the Label tag is removed, no need any more\n",
        "print ('%-17s %-30s %-10s  %-10s %-15s ' % (\"ML algorithm\",\"Feature Name\",\"F1-score\",\"Accuracy\", \"Feature List\"))# print output header\n",
        "for j in ml_list: # run for every machine learning.\n",
        "    my_list=[]\n",
        "    for i in features: ## run for every  feature\n",
        "        my_list.append(i)\n",
        "        X = df.loc[:, my_list].values # data\n",
        "\n",
        "        ## cross-validation\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "        #machine learning algorithm is applied in this section\n",
        "        clf = ml_list[j]   #\n",
        "        clf.fit(X_train, y_train)\n",
        "        predict =clf.predict(X_test)\n",
        "        f1=clf.score(X_test, y_test)\n",
        "        result=f1_score(y_test, predict, average='macro')\n",
        "        accuracy=round(clf.score(X_test, y_test),2)\n",
        "        temp=\"[\"\n",
        "\n",
        "        for ii in my_list:\n",
        "            temp+=str(my_list.index(ii)+1)+\", \" #translate property list to sequence number for less space\n",
        "\n",
        "\n",
        "        if result>=least:# If the F-criterion is equal to or greater than the highest value previously accessed, keep the new feature.\n",
        "            least=result\n",
        "            print ('%-17s %-30s %-10s  %-10s %-15s %-15s ' % (j,i,result,accuracy ,temp, \"------> New feature found!!!\"))\n",
        "\n",
        "        else:#If not, remove it from the list\n",
        "            my_list.remove(my_list[len(my_list)-1])\n",
        "            print ('%-17s %-30s %-10s  %-10s %-15s ' % (j,i,result,accuracy ,temp))\n",
        "    print(\"F1=\" ,least,j,\" The most efficient feature list =\",my_list,\"\\n\\n\") #print maximum F1 and the most efficient feature list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"operation time: = \",time.time()- seconds ,\"secomds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}